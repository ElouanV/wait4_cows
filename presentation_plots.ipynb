{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ab9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('presentation_plots')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5353e",
   "metadata": {},
   "source": [
    "## 1. Next Cow Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ad182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all next cow prediction results\n",
    "experiments = {\n",
    "    'Pure MLP': 'pure_mlp_next_cow_output',\n",
    "    'MLP + Logic': 'mlp_next_cow_output',\n",
    "    'Logic Only': 'logic_only_onehot_lr01_3000ep',\n",
    "    'One-Hot MLP': 'onehot_mlp_output',\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, path in experiments.items():\n",
    "    exp_path = Path(path)\n",
    "    if exp_path.exists():\n",
    "        # Load config\n",
    "        with open(exp_path / 'config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Load history if exists\n",
    "        history_file = exp_path / 'history.json'\n",
    "        if history_file.exists():\n",
    "            with open(history_file, 'r') as f:\n",
    "                history = json.load(f)\n",
    "            results[name] = {'config': config, 'history': history}\n",
    "        else:\n",
    "            results[name] = {'config': config, 'history': None}\n",
    "            print(f\"âš ï¸  No history found for {name}\")\n",
    "    else:\n",
    "        print(f\"âŒ {name} not found at {path}\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(results)} experiments\")\n",
    "for name in results.keys():\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Next Cow Prediction: Training Curves Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "metrics = [\n",
    "    ('train_loss', 'Training Loss', axes[0, 0]),\n",
    "    ('val_loss', 'Validation Loss', axes[0, 1]),\n",
    "    ('train_acc', 'Training Accuracy', axes[1, 0]),\n",
    "    ('val_acc', 'Validation Accuracy', axes[1, 1])\n",
    "]\n",
    "\n",
    "for metric_key, title, ax in metrics:\n",
    "    for name, data in results.items():\n",
    "        if data['history'] and metric_key in data['history']:\n",
    "            values = data['history'][metric_key]\n",
    "            # Subsample if too many epochs\n",
    "            if len(values) > 500:\n",
    "                step = len(values) // 500\n",
    "                values = values[::step]\n",
    "            ax.plot(values, label=name, linewidth=1.5, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'next_cow_training_curves.png', bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'next_cow_training_curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: next_cow_training_curves.png/.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Final performance comparison\n",
    "performance_data = []\n",
    "for name, data in results.items():\n",
    "    if data['history']:\n",
    "        # Get best validation metrics\n",
    "        best_val_acc = max(data['history']['val_acc']) if 'val_acc' in data['history'] else 0\n",
    "        best_val_loss = min(data['history']['val_loss']) if 'val_loss' in data['history'] else 0\n",
    "        final_train_acc = data['history']['train_acc'][-1] if 'train_acc' in data['history'] else 0\n",
    "        \n",
    "        performance_data.append({\n",
    "            'Model': name,\n",
    "            'Best Val Accuracy': best_val_acc,\n",
    "            'Best Val Loss': best_val_loss,\n",
    "            'Final Train Accuracy': final_train_acc\n",
    "        })\n",
    "\n",
    "df_perf = pd.DataFrame(performance_data)\n",
    "\n",
    "# Create bar plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle('Next Cow Prediction: Performance Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(df_perf))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, df_perf['Best Val Accuracy'], width, label='Val Accuracy', alpha=0.8)\n",
    "axes[0].bar(x + width/2, df_perf['Final Train Accuracy'], width, label='Train Accuracy', alpha=0.8)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Best Accuracy')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_perf['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Loss comparison\n",
    "axes[1].bar(df_perf['Model'], df_perf['Best Val Loss'], alpha=0.8)\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Best Validation Loss')\n",
    "axes[1].set_xticklabels(df_perf['Model'], rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'next_cow_performance.png', bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'next_cow_performance.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save table\n",
    "df_perf.to_csv(output_dir / 'next_cow_performance.csv', index=False)\n",
    "print(\"\\nðŸ“Š Performance Summary:\")\n",
    "print(df_perf.to_string(index=False))\n",
    "print(\"\\nâœ… Saved: next_cow_performance.png/.pdf/.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1869726",
   "metadata": {},
   "source": [
    "## 2. Graph Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph prediction results\n",
    "graph_dirs = ['graph_prediction_out', 'graph_prediction_out_bs512']\n",
    "\n",
    "graph_results = {}\n",
    "for gdir in graph_dirs:\n",
    "    exp_path = Path(gdir)\n",
    "    if exp_path.exists():\n",
    "        # Look for result files\n",
    "        result_files = list(exp_path.glob('**/metrics*.json')) + list(exp_path.glob('**/results*.json'))\n",
    "        if result_files:\n",
    "            print(f\"Found {len(result_files)} result files in {gdir}\")\n",
    "            graph_results[gdir] = result_files\n",
    "\n",
    "print(f\"\\nâœ… Found {len(graph_results)} graph prediction directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1310cc",
   "metadata": {},
   "source": [
    "## 3. Temporal Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal graph statistics\n",
    "experiments_data = {\n",
    "    'Brush': 'brush_experiment',\n",
    "    'Water Spot': 'water_spot_experiment',\n",
    "    'Lactation': 'lactation_experiment'\n",
    "}\n",
    "\n",
    "temporal_stats = {}\n",
    "for name, exp_dir in experiments_data.items():\n",
    "    exp_path = Path(exp_dir)\n",
    "    if exp_path.exists():\n",
    "        # Find summary files\n",
    "        summary_files = list(exp_path.glob('*_summary.csv'))\n",
    "        if summary_files:\n",
    "            df = pd.read_csv(summary_files[0])\n",
    "            temporal_stats[name] = df\n",
    "            print(f\"âœ… Loaded {name}: {len(df)} snapshots\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(temporal_stats)} experiment datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc301ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal graph evolution\n",
    "if temporal_stats:\n",
    "    fig, axes = plt.subplots(len(temporal_stats), 2, figsize=(12, 4*len(temporal_stats)))\n",
    "    if len(temporal_stats) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle('Temporal Graph Evolution by Experiment', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, (name, df) in enumerate(temporal_stats.items()):\n",
    "        # Plot edges over time\n",
    "        if 'num_edges' in df.columns:\n",
    "            axes[idx, 0].plot(df['num_edges'], linewidth=1.5)\n",
    "            axes[idx, 0].set_title(f'{name}: Number of Edges')\n",
    "            axes[idx, 0].set_xlabel('Time Window')\n",
    "            axes[idx, 0].set_ylabel('Number of Edges')\n",
    "            axes[idx, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot nodes over time\n",
    "        if 'num_nodes' in df.columns:\n",
    "            axes[idx, 1].plot(df['num_nodes'], linewidth=1.5, color='orange')\n",
    "            axes[idx, 1].set_title(f'{name}: Number of Active Nodes')\n",
    "            axes[idx, 1].set_xlabel('Time Window')\n",
    "            axes[idx, 1].set_ylabel('Number of Nodes')\n",
    "            axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'temporal_graph_evolution.png', bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'temporal_graph_evolution.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Saved: temporal_graph_evolution.png/.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86271e62",
   "metadata": {},
   "source": [
    "## 4. CowBERT Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cowbert results exist\n",
    "cowbert_paths = [\n",
    "    'results/cow_contrastive_out',\n",
    "    'outputs'\n",
    "]\n",
    "\n",
    "cowbert_data = {}\n",
    "for path in cowbert_paths:\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.exists():\n",
    "        # Look for result files\n",
    "        result_files = list(path_obj.glob('**/*.json')) + list(path_obj.glob('**/*.csv'))\n",
    "        if result_files:\n",
    "            print(f\"Found {len(result_files)} files in {path}\")\n",
    "            cowbert_data[path] = result_files[:10]  # Show first 10\n",
    "            for f in result_files[:5]:\n",
    "                print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nâœ… Found CowBERT data in {len(cowbert_data)} directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96981489",
   "metadata": {},
   "source": [
    "## 5. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78355fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "# Next Cow Prediction models\n",
    "for name, data in results.items():\n",
    "    if data['history']:\n",
    "        summary_data.append({\n",
    "            'Task': 'Next Cow Prediction',\n",
    "            'Model': name,\n",
    "            'Best Val Acc': f\"{max(data['history']['val_acc']):.4f}\" if 'val_acc' in data['history'] else 'N/A',\n",
    "            'Parameters': data['config'].get('num_parameters', 'N/A'),\n",
    "            'Architecture': data['config'].get('architecture', 'N/A')\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save summary\n",
    "df_summary.to_csv(output_dir / 'experiment_summary.csv', index=False)\n",
    "print(\"\\nâœ… Saved: experiment_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f2952",
   "metadata": {},
   "source": [
    "## 6. Export All Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated plots\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED PRESENTATION PLOTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {output_dir.absolute()}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "\n",
    "for file in sorted(output_dir.glob('*')):\n",
    "    size = file.stat().st_size / 1024  # KB\n",
    "    print(f\"  ðŸ“Š {file.name:50s} {size:>8.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL PLOTS GENERATED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
